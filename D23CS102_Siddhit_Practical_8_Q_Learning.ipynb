{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z3rZYsswskdA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment: Simple Grid\n",
        "class GridEnvironment:\n",
        "    def __init__(self, grid_size=(4, 4), start=(0, 0), goal=(3, 3)):\n",
        "        self.grid_size = grid_size\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.state = start\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            raise Exception(\"Episode is done. Please reset the environment.\")\n",
        "\n",
        "        next_state = list(self.state)\n",
        "\n",
        "        if action == 0:   # Up\n",
        "            next_state[0] = max(0, self.state[0] - 1)\n",
        "        elif action == 1: # Down\n",
        "            next_state[0] = min(self.grid_size[0] - 1, self.state[0] + 1)\n",
        "        elif action == 2: # Left\n",
        "            next_state[1] = max(0, self.state[1] - 1)\n",
        "        elif action == 3: # Right\n",
        "            next_state[1] = min(self.grid_size[1] - 1, self.state[1] + 1)\n",
        "\n",
        "        self.state = tuple(next_state)\n",
        "\n",
        "        if self.state == self.goal:\n",
        "            self.done = True\n",
        "            reward = 1  # Reward for reaching the goal\n",
        "        else:\n",
        "            reward = -0.1  # Small penalty for each step\n",
        "\n",
        "        return self.state, reward, self.done\n",
        "\n",
        "    def render(self):\n",
        "        for i in range(self.grid_size[0]):\n",
        "            for j in range(self.grid_size[1]):\n",
        "                if (i, j) == self.state:\n",
        "                    print(\"A\", end=\" \")  # Agent position\n",
        "                elif (i, j) == self.goal:\n",
        "                    print(\"G\", end=\" \")  # Goal position\n",
        "                else:\n",
        "                    print(\".\", end=\" \")  # Empty space\n",
        "            print()  # New line after each row\n",
        "        print()  # Extra line for better readability"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning Algorithm\n",
        "class QLearningAgent:\n",
        "    def __init__(self, actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.99, min_exploration_rate=0.01):\n",
        "        self.q_table = {}\n",
        "        self.actions = actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.min_exploration_rate = min_exploration_rate\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        best_next_q = max(self.get_q_value(next_state, a) for a in range(self.actions))\n",
        "        current_q = self.get_q_value(state, action)\n",
        "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * best_next_q - current_q)\n",
        "        self.q_table[(state, action)] = new_q\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return random.choice(range(self.actions))  # Explore\n",
        "        else:\n",
        "            return np.argmax([self.get_q_value(state, a) for a in range(self.actions)])  # Exploit\n",
        "\n",
        "    def decay_exploration(self):\n",
        "        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)"
      ],
      "metadata": {
        "id": "xZSjx5IqtxI-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop to run Q-learning\n",
        "def train_agent(episodes=1000):\n",
        "    env = GridEnvironment()\n",
        "    agent = QLearningAgent(actions=4)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        while True:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.update_q_value(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.decay_exploration()\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "QXP0YPR_t1SS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    trained_agent = train_agent(episodes=5000)\n",
        "\n",
        "    # Test the trained agent\n",
        "    env = GridEnvironment()\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = trained_agent.choose_action(state)\n",
        "        state, reward, done = env.step(action)\n",
        "        total_reward += reward\n",
        "        env.render()  # Implement a render function to visualize the environment if needed\n",
        "\n",
        "        if done:\n",
        "            print(f\"Total Reward: {total_reward}\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-2Px7Bkt5Yz",
        "outputId": "77a445d4-d5fe-4108-84b5-01b0de41bcef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". A . . \n",
            ". . . . \n",
            ". . . . \n",
            ". . . G \n",
            "\n",
            ". . . . \n",
            ". A . . \n",
            ". . . . \n",
            ". . . G \n",
            "\n",
            ". . . . \n",
            ". . . . \n",
            ". A . . \n",
            ". . . G \n",
            "\n",
            ". . . . \n",
            ". . . . \n",
            ". . A . \n",
            ". . . G \n",
            "\n",
            ". . . . \n",
            ". . . . \n",
            ". . . . \n",
            ". . A G \n",
            "\n",
            ". . . . \n",
            ". . . . \n",
            ". . . . \n",
            ". . . A \n",
            "\n",
            "Total Reward: 0.5\n"
          ]
        }
      ]
    }
  ]
}